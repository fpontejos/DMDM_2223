{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load csv file into a dataframe \n",
    "data_path = \"https://raw.githubusercontent.com/fpontejos/DMDM_2223/main/data/datamining.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make copy of original data\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric and non-metric features. Why?\n",
    "non_metric_features = [\"education\", \"status\", \"gender\", \"dependents\", \"description\"]\n",
    "metric_features = df.columns.drop(non_metric_features).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace empty string with NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get modes of non metric features\n",
    "modes = df_central[non_metric_features].mode().loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace NANs with median (metric features) or modes (non metric features)\n",
    "\n",
    "df_central.fillna(df_central.median(), inplace=True)\n",
    "df_central.fillna(modes, inplace=True)\n",
    "\n",
    "#df_central.isna().sum()  # checking how many NaNs we still have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_central.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may vary from session to session, and is prone to varying interpretations.\n",
    "# A simple example is provided below:\n",
    "\n",
    "filters1 = (\n",
    "    (df['house_keeping']<=50)\n",
    "    &\n",
    "    (df['kitchen']<=40)\n",
    "    &\n",
    "    (df['toys']<=35)\n",
    "    &\n",
    "    (df['education']!='OldSchool')\n",
    ")\n",
    "\n",
    "df_1 = df[filters1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['birth_year'] = df['age']\n",
    "df['age'] = datetime.now().year - df['birth_year']\n",
    "\n",
    "df['spent_online'] = (df['per_net_purchase'] / 100) * df['mnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace extreme values in 'rcn' with threshold value 100\n",
    "print((df['rcn']>100).value_counts())\n",
    "\n",
    "rcn_t = df['rcn'].copy()\n",
    "rcn_t.loc[rcn_t>100] = 100\n",
    "\n",
    "df['rcn'] = rcn_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables according to their correlations\n",
    "df.drop(columns=['birth_year', 'age', 'mnt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating metric_features\n",
    "metric_features.append(\"spent_online\")\n",
    "metric_features.remove(\"mnt\")\n",
    "metric_features.remove(\"age\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_feat = scaler.fit_transform(df_standard[metric_features])\n",
    "df_standard[metric_features] = scaled_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_standard.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encode features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's remove status=Whatever\n",
    "df_ohc.loc[df_ohc['status'] == 'Whatever', 'status'] = df['status'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical features. Get feature names and create a DataFrame \n",
    "# with the one-hot encoded categorical features (pass feature names)\n",
    "ohc = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "ohc_feat = ohc.fit_transform(df_ohc[non_metric_features])\n",
    "ohc_feat_names = ohc.get_feature_names()\n",
    "ohc_df = pd.DataFrame(ohc_feat, index=df_ohc.index, columns=ohc_feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain ohc variables\n",
    "df_ohc = pd.concat([df_ohc.drop(columns=non_metric_features), ohc_df], axis=1)\n",
    "df_ohc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with the number of principal components you want to retain\n",
    "pca = PCA(n_components=3)\n",
    "pca_feat = pca.fit_transform(df_pca[metric_features])\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "df_pca = pd.concat([df_pca, pca_df], axis=1)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR... Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/fpontejos/DMDM_2223/main/data/tugas_preprocessed.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OR if loading from CSV in your Google Drive\n",
    "## Uncomment these lines:\n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#df_drive_path = \"/content/drive/MyDrive/Colab Notebooks/tugas_preprocessed.csv\"\n",
    "#df = pd.read_csv(df_drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "non_metric_features = df.columns[df.columns.str.startswith('x')]\n",
    "pc_features = df.columns[df.columns.str.startswith('PC')]\n",
    "metric_features = df.columns[~df.columns.str.startswith('x') & ~df.columns.str.startswith('PC')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "What is hierarchical clustering? How does it work? How does it relate to the distance matrix?\n",
    "\n",
    "### Different types of linkage\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png)\n",
    "\n",
    "### How are they computed?\n",
    "![](https://raw.githubusercontent.com/fpontejos/DMDM_2223/main/figures/linkage_types.jpeg)\n",
    "\n",
    "**Ward linkage**: minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "\n",
    "### The distance matrix\n",
    "![](https://raw.githubusercontent.com/fpontejos/DMDM_2223/main/figures/hc_distance_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics:\n",
    "- *bottom up approach*: each observation starts in its own cluster, and clusters are successively merged together\n",
    "- *greedy/local algorithm*: at each iteration tries to minimize the distance of cluster merging\n",
    "- *no realocation*: after an observation is assigned to a cluster, it can no longer change\n",
    "- *deterministic*: you always get the same answer when you run it\n",
    "- *scalability*: can become *very slow* for a large number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to apply Hierarchical Clustering?\n",
    "**Note: Which types of variables should be used for clustering?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing HC\n",
    "hclust = AgglomerativeClustering(linkage='ward', affinity='euclidean', n_clusters=5)\n",
    "hc_labels = hclust.fit_predict(df[metric_features])\n",
    "hc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the clusters\n",
    "df_concat = pd.concat((df, pd.Series(hc_labels, name='labels')), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the linkage method to choose:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to understand that:**\n",
    "$$SS_{t} = SS_{w} + SS_{b}$$\n",
    "\n",
    "---\n",
    "\n",
    "$$SS_{t} = \\sum\\limits_{i = 1}^n {{{({x_i} - \\overline x )}^2}}$$\n",
    "\n",
    "$$SS_{w} = \\sum\\limits_{k = 1}^K {\\sum\\limits_{i = 1}^{{n_k}} {{{({x_i} - {{\\overline x }_k})}^2}} }$$\n",
    "\n",
    "$$SS_{b} = \\sum\\limits_{k = 1}^K {{n_k}{{({{\\overline x }_k} - \\overline x )}^2}}$$\n",
    "\n",
    ", where $n$ is the total number of observations, $x_i$ is the vector of the $i^{th}$ observation, $\\overline x$ is the centroid of the data, $K$  is the number of clusters, $n_k$ is the number of observations in the $k^{th}$ cluster and $\\overline x_k$ is the centroid of the $k^{th}$ cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing SST\n",
    "X = df[metric_features].values\n",
    "sst = np.sum(np.square(X - X.mean(axis=0)), axis=0)\n",
    "\n",
    "# Computing SSW\n",
    "ssw_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssw_iter.append(np.sum(np.square(X_k - X_k.mean(axis=0)), axis=0))\n",
    "ssw = np.sum(ssw_iter, axis=0)\n",
    "\n",
    "# Computing SSB\n",
    "ssb_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssb_iter.append(X_k.shape[0] * np.square(X_k.mean(axis=0) - X.mean(axis=0)))\n",
    "ssb = np.sum(ssb_iter, axis=0)\n",
    "\n",
    "# Verifying the formula\n",
    "np.round(sst) == np.round((ssw + ssb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_hc(df, link_method, max_nclus, min_nclus=1, dist=\"euclidean\"):\n",
    "    \"\"\"This function computes the R2 for a set of cluster solutions given by the application of a hierarchical method.\n",
    "    The R2 is a measure of the homogenity of a cluster solution. It is based on SSt = SSw + SSb and R2 = SSb/SSt. \n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to apply clustering\n",
    "    link_method (str): either \"ward\", \"complete\", \"average\", \"single\"\n",
    "    max_nclus (int): maximum number of clusters to compare the methods\n",
    "    min_nclus (int): minimum number of clusters to compare the methods. Defaults to 1.\n",
    "    dist (str): distance to use to compute the clustering solution. Must be a valid distance. Defaults to \"euclidean\".\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: R2 values for the range of cluster solutions\n",
    "    \"\"\"\n",
    "    def get_ss(df):\n",
    "        ss = np.sum(df.var() * (df.count() - 1))\n",
    "        return ss  # return sum of sum of squares of each df variable\n",
    "    \n",
    "    sst = get_ss(df)  # get total sum of squares\n",
    "    \n",
    "    r2 = []  # where we will store the R2 metrics for each cluster solution\n",
    "    \n",
    "    for i in range(min_nclus, max_nclus+1):  # iterate over desired ncluster range\n",
    "        cluster = AgglomerativeClustering(n_clusters=i, affinity=dist, linkage=link_method)\n",
    "        \n",
    "        \n",
    "        hclabels = cluster.fit_predict(df) #get cluster labels\n",
    "        \n",
    "        \n",
    "        df_concat = pd.concat((df, pd.Series(hclabels, name='labels')), axis=1)  # concat df with labels\n",
    "        \n",
    "        \n",
    "        ssw_labels = df_concat.groupby(by='labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "        \n",
    "        \n",
    "        ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "        \n",
    "        \n",
    "        r2.append(ssb / sst)  # save the R2 of the given cluster solution\n",
    "        \n",
    "    return np.array(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function plots the R2 values for each linkage method for cluster sizes from 1 to 10\n",
    "\n",
    "def plot_r2_linkage(df, max_nclus):\n",
    "    # Prepare input\n",
    "    hc_methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "    # Call function defined above to obtain the R2 statistic for each hc_method\n",
    "    max_nclus = 10\n",
    "    r2_hc_methods = np.vstack(\n",
    "        [\n",
    "            get_r2_hc(df=df, link_method=link, max_nclus=max_nclus) \n",
    "            for link in hc_methods\n",
    "        ]\n",
    "    ).T\n",
    "    r2_hc_methods = pd.DataFrame(r2_hc_methods, index=range(1, max_nclus + 1), columns=hc_methods)\n",
    "\n",
    "    sns.set()\n",
    "    # Plot data\n",
    "    fig = plt.figure(figsize=(11,5))\n",
    "    sns.lineplot(data=r2_hc_methods, linewidth=2.5, markers=[\"o\"]*4)\n",
    "\n",
    "    # Finalize the plot\n",
    "    fig.suptitle(\"R2 plot for various hierarchical methods\", fontsize=21)\n",
    "    plt.gca().invert_xaxis()  # invert x axis\n",
    "    plt.legend(title=\"HC methods\", title_fontsize=11)\n",
    "    plt.xticks(range(1, max_nclus + 1))\n",
    "    plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "    plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nclus = 10\n",
    "plot_r2_linkage(df[metric_features], max_nclus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the number of clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is the **first big jump** on the Dendrogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 and n_clusters=None ensures we compute the full tree\n",
    "\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "\n",
    "hclust = AgglomerativeClustering(linkage=linkage, \n",
    "                                 affinity=distance, \n",
    "                                 distance_threshold=0, \n",
    "                                 n_clusters=None)\n",
    "\n",
    "hclust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "    \n",
    "    \n",
    "## Create linkage matrix\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "\n",
    "\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 100\n",
    "\n",
    "\n",
    "# \"dendrogram\" function will plot our dendrogram\n",
    "dendrogram(linkage_matrix, \n",
    "           truncate_mode='level', \n",
    "           p=5, \n",
    "           color_threshold=y_threshold, \n",
    "           above_threshold_color='k')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Hierarchical clustering solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 cluster solution\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "h4clust = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=4)\n",
    "hc4_labels = h4clust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 4 clusters\n",
    "df_concat = pd.concat((df, pd.Series(hc4_labels, name='hc4_labels')), axis=1)\n",
    "df_concat.groupby('hc4_labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 cluster solution\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "hc5lust = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=5)\n",
    "hc5_labels = hc5lust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 5 clusters\n",
    "df_concat = pd.concat((df_concat, pd.Series(hc5_labels, name='hc5_labels')), axis=1)\n",
    "df_concat.groupby('hc5_labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_concat['hc5_labels'],\n",
    "           df_concat['hc4_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
